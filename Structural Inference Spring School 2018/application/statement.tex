\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{graphicx}%
\usepackage{fancyhdr}

\theoremstyle{plain} \numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{finalremark}[theorem]{Final Remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}{Question} \topmargin-2cm

\textwidth6in

\setlength{\topmargin}{0in} \addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}

\setlength{\oddsidemargin}{0in}

\oddsidemargin  0.0in \evensidemargin 0.0in \parindent0em

\pagestyle{fancy}\lhead{Research Statement} \rhead{\today}
\chead{{\large{\bf Xuedong Shang}}} \lfoot{} \rfoot{\bf \thepage} \cfoot{}

\newcounter{list}

\newcommand\institute[1]{\large \textbf{#1}}

\begin{document}

\title{Research Statement}
\author{Xuedong Shang\\ \large  \textbf{Inria Lille-Nord Europe}}
\maketitle

\section{Introduction}

Optimization in stochastic environments is an active research topic both in mathematic and computer science community with a lot of applications in different fields such as finance~\cite{ziemba2006}, biology and chemistry~\cite{floudas2000}, engineering~\cite{wang2007}, bioinformatics~\cite{moles2003}, etc. For my PhD thesis, I will be particularly interested in optimization of functions for which none (or few) regularity assumptions are made, and only noisy function evaluations can be observed. This is the "black-box" global optimization problem.

In this context, since one does not make extra regularity assumptions on the target function, one can imagine that it can be very costly to evaluate the function. Thus a good strategy for choosing adaptively the next observation is needed in order to find an optimal (or quasi-optimal) point with as few number of evaluations as possible. This is the sequential optimization problem. That being said, the main problematic of my thesis is the global sequential optimization problem.

Recently this kind of black box sequential optimization is motived in particular by applications in automatic hyperparameter configuration of machine learning algorithms~\cite{hoffman2014}. One may also think about the planning problem for Markov Decision Processes~\cite{puterman1994}, for which the objective is, for a given state, to decide which action to take that maximises a fonction value along with the noisy observations we get for some well choosen paths.

\section{Multi-armed Bandits}

In the past few years, such optimization and planning problems have been widely inspired by literature of multi-armed bandit (MAB) algorithms (see~\cite{munos2014} for a survey). These algorithms are based on a hierarchical exploration (in a tree form) of the domain of the function, with the help of the optimism principle for choosing which part of the tree to explore. These works brought some breakthroughs especially for Monte-Carlo Tree Search (MCTS), which led to some great improvements in AI designing, e.g. for the game of Go~\cite{silver2016}.

A simple way to describe the multi-armed bandit scenario is to consider $K$ arms labeled by integers from $1$ to $K$. Each arm $k\in\{1,\ldots,K\}$ is characterized by an unknown distribution $\nu_k$. At each step $t$, an arm $k_t$ is selected and some reward $r_t\sim\nu_{k_t}$ is returned. The optimism principle is used when we are interested in maximizing the sum of rewards. Meanwhile, another objective could have been preferred, which is to decide as quickly as possible which arm has the highest reward on average. Recent works showed that optimal algorithms for this kind of best arm identification problems are totally different from those for reward maximization problems~\cite{bubeck2011b,kaufmann2017}. This put into question the use of optimistic paradigm for some sequential optimization problems.

\section{Perspectives}

Dealing with sequential global optimization using hierarchical exploration with the help of best arm identification techniques instead of the classic optimistic approaches has raised much attention recently. For instance, most existing algorithms for the MCTS problem are variants of the Upper-Confidence Tree (UCT) algorithm in~\cite{kocsis2006} in which the exploration phase follows the optimism in the face of uncertainty principle. The goal is to use a different approach, in which the exploration phase will be based on a process of best arm identification inspired by algorithms like LUCB~\cite{kaly2012}, UGapE~\cite{gabillon2012}, Successive Reject~\cite{audibert2010}, combining with  methods of hierarchical optimization (e.g. hierarchical optimistic optimization~\cite{grill2015,bubeck2011a}) and methods using Gaussian processes (e.g. Bayesian optimization~\cite{brochu2010}). 

\section{Applications}

Not only interested in the theorectical part, I am also working on some interesting applications of sequential global optimization, e.g. hyperparameter optimization and algorithm selection.

\subsection{Hyperparameter Optimization}

One important application of black box optimization is hyperparameter optimization in the context of machine learning. Hyperparameter optimization is usually the most tedious part in fitting a machine learning algorithm. In practice, we are interested in models whose loss, evaluated on an independent part of data is as low as possible. Different hyperparameter choices lead to different losses, therefore finding the optimal set of hyperparameters is of importance. In reality, we never know that if the point found is the global optimum, but from a practical point of view, we are only interested in finding the model that works best in a production setting.

In this context, we consider a function $f:\mathcal{X}\rightarrow\mathbb{R}$ and take $\mathcal{X}$ as a set of parameter configurations (set of arms), we can then map one parameter configuration to the performance of the machine learning classifier using the corresponding parameters.

\subsection{Algorithm Selection}

Many AI-problems are NP-complete. Nevertheless, many practical problems are solved efficiently by employing powerful heuristics. Such heuristics work well in some cases, but not in others. Due to their complex nature, it is difficult to identify which conditions are necessary for a heuristic to perform well and, more generally, to identify the algorithm that is best suited for solving a given instance. This is known as the algorithm selection problem.

In this context, an algorithm is considered as an arm, and the action of pulling an arm consists in running a chosen algorithm on some problem instance. In collaboration with Hans Degroote, we have a working paper on a special scenario of this problem, where instances will be arriving online and we have access to some features of each instance. This leads to another setting of MAB called contextual multi-armed bandit where some side information are available for each arm.

\begin{thebibliography}{99}

\bibitem{ziemba2006}
\newblock Ziemba, W. T. and Vickson, R. G.
\newblock Stochastic optimization models in finance.
\newblock {\em World
Scientific Singapore}, 2006.

\bibitem{floudas2000}
\newblock Floudas, C. A. and Pardalos, P. M.
\newblock Optimization in computational chemistry and molecular
biology: Local and global approaches.
\newblock {\em Nonconvex Optimization and Its Applications.
Springer}, 2000.

\bibitem{wang2007}
\newblock Wang, G. and Shan, S.
\newblock Review of metamodeling techniques in support of engineering
design optimization.
\newblock {\em Journal of Mechanical Design}, 129(4):370-380, 2007.

\bibitem{moles2003}
\newblock Moles, C. G., Mendes, P. and Banga, J. R.
\newblock Parameter estimation in biochemical
pathways: a comparison of global optimization methods.
\newblock {\em Genome Research}, 13(11):2467-2474, 1996.

\bibitem{hoffman2014}
\newblock Hoffman, M., Shahriari, B. and de Freitas, N.
\newblock On correlation and budget constraints in model-basedbandit optimization with application to automatic machine learning.
\newblock {\em In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AIStats)}, 2014.

\bibitem{puterman1994}
\newblock Puterman, M. L.
\newblock Markov decision processes.
\newblock {\em Discrete Stochastic Dynamic Programming}, Wiley, 1994.

\bibitem{munos2014}
\newblock Munos, R.
\newblock From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning.
\newblock {\em Foundations and Trends in Machine Learning}, 7, 2014.

\bibitem{kocsis2006}
\newblock Kocsis, L. and Szepesv\'ari, C.
\newblock Bandit based monte-carlo planning.
\newblock {\em In Proceedings of the 17th European Conference on Machine Learning (ECML)}, 282-293, 2006.

\bibitem{kaly2012}
\newblock Kalyanakrishnan, S., Tewari, A. and Auer, P. and Stone, P.
\newblock PAC subset selection in stochastic multi-armed bandits.
\newblock {\em In International Conference on Machine Learning (ICML)}, 2012.

\bibitem{bubeck2011a}
\newblock Bubeck, S., Munos, R., Stoltz, G. and Szepesv\'ari C.
\newblock X-armed bandits.
\newblock {\em Machine Learning Research}, 12:1587-1627, 2011.

\bibitem{grill2015}
\newblock Grill, J.-B., Valko, M. and Munos, R.
\newblock Black-box optimization of noisy functions with unknown smoothness.
\newblock {\em In Neural Information Processing Systems (NIPS)}, 2015.

\bibitem{brochu2010}
\newblock Brochu, E., Cora, V. M. and de Freitas, N.
\newblock A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.
\newblock {\em Technical report, University of Bristish Columbia}, 2010.

\bibitem{silver2016}
\newblock Silver, D. et al.
\newblock Mastering the game of go with deep neuralnetworks and tree search.
\newblock {\em Nature}, 529:484-489, 2016.

\bibitem{bubeck2011b}
\newblock Bubeck, S., Munos, R. and Stoltz, G.
\newblock Pure exploration in finitely armed and continuous armed bandits.
\newblock {\em Theoretical Computer Science}, 412:1832â€“1852, 2011.

\bibitem{kaufmann2017}
\newblock Kaufmann, E. and Garivier, A.
\newblock Learning the distribution with highest mean : two bandit frameworks.
\newblock {\em To appear in Mathematical Modelling and Numerical Analysis (ESAIM): Proceedings and Survey}, 2017.

\bibitem{gabillon2012}
\newblock Gabillon, V., Ghavamzadeh, M. and Lazaric, A.
\newblock Best arm identification: A unified approach to fixed budget and fixed confidence.
\newblock {\em In Advances in Neural Information Processing Systems}, 2012.

\bibitem{audibert2010}
\newblock Audibert, J.-Y., Bubeck, S. and Munos, R.
\newblock Best arm identification in multi-armed bandits.
\newblock {\em In Annual Conference on Learning Theory}, 2010.


\end{thebibliography}

\end{document}


